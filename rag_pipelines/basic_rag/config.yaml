llm:
    model_id: "meta-llama/llama-4-scout-17b-16e-instruct"
    max_tokens: 500
    temperature: 0
    top_p: 0.1
    stop_sequences: []
vector_db:
    embedding_modelname: "intfloat/multilingual-e5-small"
    collection_name: "chroma_collection_basic_rag"
    persist_directory: "./chroma_db"
chunking:
    chunk_size: 512
    chunk_overlap: 50
    separators: ["\n\n", " "]
file_reader:
    pdf:
        pdfloader: "PYMUPDF"
    docx:
        docxloader: "python-docx"